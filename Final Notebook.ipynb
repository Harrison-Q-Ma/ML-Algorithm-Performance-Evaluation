{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing needed packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from warnings import simplefilter\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of datasets\n",
    "df_adult_income = pd.read_csv('adult.data', sep=\",\", names = ['age','workclass','fnlwgt','education','education-num','marital-status','occupation','relationship','race','sex','capital-gain','capital-loss','hours-per-week','native-country','class'])\n",
    "df_krk = pd.read_csv('krkopt.data', sep=\",\", names=['wkf','wkr','wrf','wrr','bkf','bkr','outcome'])\n",
    "df_connect_4 = pd.read_csv('connect-4.data', sep=\",\")\n",
    "df_HTRU_2 = pd.read_csv('HTRU_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning for census income data\n",
    "df_adult_income_enc = pd.DataFrame(df_adult_income['age'])\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc_df = pd.DataFrame(enc.fit_transform(df_adult_income[['workclass']]).toarray())\n",
    "df_adult_income_enc = df_adult_income_enc.join(enc_df)\n",
    "df_adult_income_enc = df_adult_income_enc.join(df_adult_income['fnlwgt'])\n",
    "\n",
    "enc_df = pd.DataFrame(enc.fit_transform(df_adult_income[['education']]).toarray())\n",
    "df_adult_income_enc = pd.concat([df_adult_income_enc, enc_df], axis = 1)\n",
    "df_adult_income_enc = df_adult_income_enc.join(df_adult_income['education-num'])\n",
    "\n",
    "enc_df = pd.DataFrame(enc.fit_transform(df_adult_income[['marital-status']]).toarray())\n",
    "df_adult_income_enc = pd.concat([df_adult_income_enc, enc_df], axis = 1)\n",
    "\n",
    "enc_df = pd.DataFrame(enc.fit_transform(df_adult_income[['occupation','relationship','sex','race']]).toarray())\n",
    "df_adult_income_enc = pd.concat([df_adult_income_enc, enc_df], axis = 1)\n",
    "\n",
    "df_adult_income_enc = pd.concat([df_adult_income_enc,df_adult_income['capital-gain']], axis = 1)\n",
    "df_adult_income_enc = pd.concat([df_adult_income_enc,df_adult_income['capital-loss']], axis = 1)\n",
    "df_adult_income_enc = pd.concat([df_adult_income_enc,df_adult_income['hours-per-week']], axis = 1)\n",
    "\n",
    "enc_df = pd.DataFrame(enc.fit_transform(df_adult_income[['native-country']]).toarray())\n",
    "df_adult_income_enc = pd.concat([df_adult_income_enc, enc_df], axis = 1)\n",
    "\n",
    "df_adult_income_enc = df_adult_income_enc.join(df_adult_income['class'])\n",
    "\n",
    "le = LabelEncoder() \n",
    "df_adult_income_enc['class']= le.fit_transform(df_adult_income['class']) \n",
    "\n",
    "df_adult_income = df_adult_income_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning for king-rook versus king\n",
    "# we did not use one-hot encoding because of the inherent structure to the board layout\n",
    "df_krk = df_krk.replace('a',1)\n",
    "df_krk = df_krk.replace('b',2)\n",
    "df_krk = df_krk.replace('c',3)\n",
    "df_krk = df_krk.replace('d',4)\n",
    "df_krk = df_krk.replace('e',5)\n",
    "df_krk = df_krk.replace('f',6)\n",
    "df_krk = df_krk.replace('g',7)\n",
    "df_krk = df_krk.replace('h',8)\n",
    "df_krk = df_krk.replace('draw',-1)\n",
    "df_krk['outcome'] = df_krk['outcome'].replace(['zero','one','two','three','four','five','six','seven','eight','nine','ten','eleven','twelve','thirteen','fourteen','fifteen','sixteen'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning for connect-4\n",
    "df_connect_4 = df_connect_4.replace('b',0)\n",
    "df_connect_4 = df_connect_4.replace('x',1)\n",
    "df_connect_4 = df_connect_4.replace('o',-1)\n",
    "df_connect_4 = df_connect_4.replace('draw',-1)\n",
    "df_connect_4 = df_connect_4.replace('loss',-1)\n",
    "df_connect_4 = df_connect_4.replace('win',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first deal with the HTRU_2 DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.979\n",
      "0.9714\n",
      "0.975\n",
      "0.9746\n",
      "0.9758\n"
     ]
    }
   ],
   "source": [
    "# parameter search for Neural Network for HTRU_2\n",
    "param = {'hidden_layer_sizes':[1,2,4,8,32,128],'momentum':[0,0.2,0.5,0.9]}\n",
    "model = MLPClassifier()\n",
    "\n",
    "accu_score = [] \n",
    "roc_score = [] \n",
    "f1_score = [] \n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    data_sample = df_HTRU_2.sample(5000)\n",
    "    \n",
    "    X = data_sample.iloc[:,:len(data_sample.columns)-1]\n",
    "    y = data_sample.iloc[:,len(data_sample.columns)-1:len(data_sample.columns)]\n",
    "    y = np.ravel(y)\n",
    "\n",
    "    clf = GridSearchCV(model, param, scoring = ['accuracy','roc_auc_ovr','f1_micro'], refit = False, n_jobs = -1)\n",
    "    best_model = clf.fit(X,y)   \n",
    "    \n",
    "    best_accu = best_model.cv_results_['params'][np.argmin(best_model.cv_results_['rank_test_accuracy'])]\n",
    "    best_roc = best_model.cv_results_['params'][np.argmin(best_model.cv_results_['rank_test_roc_auc_ovr'])]\n",
    "    best_f1 = best_model.cv_results_['params'][np.argmin(best_model.cv_results_['rank_test_f1_micro'])]\n",
    "    \n",
    "    X = df_HTRU_2.iloc[:,:len(df_HTRU_2.columns)-1]\n",
    "    y = df_HTRU_2.iloc[:,len(df_HTRU_2.columns)-1:len(df_HTRU_2.columns)]\n",
    "    y = np.ravel(y)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,train_size = 5000)\n",
    "    \n",
    "    predicteds_accu = [] \n",
    "    predicteds_roc = [] \n",
    "    predicteds_f1 = [] \n",
    "    trueys =[]\n",
    "    \n",
    "    model_accu = MLPClassifier(hidden_layer_sizes = best_accu['hidden_layer_sizes'], momentum = best_accu['momentum'])\n",
    "    model_roc = MLPClassifier(hidden_layer_sizes = best_roc['hidden_layer_sizes'], momentum = best_roc['momentum'])\n",
    "    model_f1 = MLPClassifier(hidden_layer_sizes = best_f1['hidden_layer_sizes'], momentum = best_f1['momentum'])\n",
    "    model_accu.fit(X_train,y_train)\n",
    "    model_roc.fit(X_train,y_train)\n",
    "    model_f1.fit(X_train,y_train)\n",
    "    \n",
    "    print(model_accu.score(X_train,y_train))\n",
    "    \n",
    "    predicteds_accu.append(model_accu.predict(X_test))\n",
    "    predicteds_roc.append(model_roc.predict(X_test))\n",
    "    predicteds_f1.append(model_f1.predict(X_test))\n",
    "    trueys.append(y_test)\n",
    "\n",
    "    predicteds_accu = np.concatenate(predicteds_accu)\n",
    "    predicteds_roc = np.concatenate(predicteds_roc)\n",
    "    predicteds_f1 = np.concatenate(predicteds_f1)\n",
    "    trueys = np.concatenate(trueys)\n",
    "    \n",
    "    accu_score.append(accuracy_score(trueys,predicteds_accu))\n",
    "    roc_score.append(accuracy_score(trueys,predicteds_roc))\n",
    "    f1_score.append(accuracy_score(trueys,predicteds_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9751104908118167, 0.9746452663410096, 0.9755757152826239, 0.9759634023416298, 0.9748003411646119]\n",
      "[0.9744901915174071, 0.9758083275180275, 0.9734822051639916, 0.9759634023416298, 0.9763510894006358]\n",
      "[0.976428626812437, 0.9763510894006358, 0.9729394432813833, 0.9757307901062262, 0.9755757152826239]\n"
     ]
    }
   ],
   "source": [
    "print(accu_score)\n",
    "print(roc_score)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9792\n",
      "0.978\n",
      "0.981\n",
      "0.978\n",
      "0.9816\n"
     ]
    }
   ],
   "source": [
    "# parameter search for Logistic Regression for HTRU_2\n",
    "param = {'penalty':('l2','none'), 'C':[0.000001,0.000001,0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000],'solver': ['newton-cg']}\n",
    "model = model = LogisticRegression()\n",
    "\n",
    "accu_score = [] \n",
    "roc_score = [] \n",
    "f1_score = [] \n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    data_sample = df_HTRU_2.sample(5000)\n",
    "    simplefilter(action='ignore', category=UserWarning)\n",
    "    \n",
    "    X = data_sample.iloc[:,:len(data_sample.columns)-1]\n",
    "    y = data_sample.iloc[:,len(data_sample.columns)-1:len(data_sample.columns)]\n",
    "    y = np.ravel(y)\n",
    "\n",
    "    clf = GridSearchCV(model, param, scoring = ['accuracy','roc_auc_ovr','f1_micro'], refit = False, n_jobs = -1)\n",
    "    best_model = clf.fit(X,y)   \n",
    "    \n",
    "    best_accu = best_model.cv_results_['params'][np.argmin(best_model.cv_results_['rank_test_accuracy'])]\n",
    "    best_roc = best_model.cv_results_['params'][np.argmin(best_model.cv_results_['rank_test_roc_auc_ovr'])]\n",
    "    best_f1 = best_model.cv_results_['params'][np.argmin(best_model.cv_results_['rank_test_f1_micro'])]\n",
    "    \n",
    "    X = df_HTRU_2.iloc[:,:len(df_HTRU_2.columns)-1]\n",
    "    y = df_HTRU_2.iloc[:,len(df_HTRU_2.columns)-1:len(df_HTRU_2.columns)]\n",
    "    y = np.ravel(y)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,train_size = 5000)\n",
    "    \n",
    "    predicteds_accu = [] \n",
    "    predicteds_roc = [] \n",
    "    predicteds_f1 = [] \n",
    "    trueys =[]\n",
    "    \n",
    "    model_accu = LogisticRegression(penalty = best_accu['penalty'], C = best_accu['C'], solver = 'newton-cg')\n",
    "    model_roc = LogisticRegression(penalty = best_roc['penalty'], C = best_roc['C'], solver = 'newton-cg')\n",
    "    model_f1 = LogisticRegression(penalty = best_f1['penalty'], C = best_f1['C'], solver = 'newton-cg')\n",
    "    model_accu.fit(X_train,y_train)\n",
    "    model_roc.fit(X_train,y_train)\n",
    "    model_f1.fit(X_train,y_train)\n",
    "    \n",
    "    print(model_accu.score(X_train,y_train))\n",
    "    \n",
    "    predicteds_accu.append(model_accu.predict(X_test))\n",
    "    predicteds_roc.append(model_roc.predict(X_test))\n",
    "    predicteds_f1.append(model_f1.predict(X_test))\n",
    "    trueys.append(y_test)\n",
    "\n",
    "    predicteds_accu = np.concatenate(predicteds_accu)\n",
    "    predicteds_roc = np.concatenate(predicteds_roc)\n",
    "    predicteds_f1 = np.concatenate(predicteds_f1)\n",
    "    trueys = np.concatenate(trueys)\n",
    "    \n",
    "    accu_score.append(accuracy_score(trueys,predicteds_accu))\n",
    "    roc_score.append(accuracy_score(trueys,predicteds_roc))\n",
    "    f1_score.append(accuracy_score(trueys,predicteds_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9796851981080872, 0.9807707218733038, 0.9775141505776537, 0.9790648988136776, 0.9785996743428704]\n",
      "[0.9782119872838645, 0.979995347755292, 0.9772040009304489, 0.9786772117546716, 0.9774366131658525]\n",
      "[0.9796851981080872, 0.9807707218733038, 0.9775141505776537, 0.9790648988136776, 0.9785996743428704]\n"
     ]
    }
   ],
   "source": [
    "print(accu_score)\n",
    "print(roc_score)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99\n",
      "0.9854\n",
      "0.9926\n",
      "0.9864\n",
      "0.9862\n"
     ]
    }
   ],
   "source": [
    "# parameter search for Random Forest for HTRU_2\n",
    "param = {'n_estimators':[1024], 'max_features':[1,2,4,6,8], 'min_samples_split':[2,5,10], 'min_samples_leaf':[1,2,4]}\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "accu_score = [] \n",
    "roc_score = [] \n",
    "f1_score = [] \n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    data_sample = df_HTRU_2.sample(5000)\n",
    "    simplefilter(action='ignore', category=UserWarning)\n",
    "    \n",
    "    X = data_sample.iloc[:,:len(data_sample.columns)-1]\n",
    "    y = data_sample.iloc[:,len(data_sample.columns)-1:len(data_sample.columns)]\n",
    "    y = np.ravel(y)\n",
    "\n",
    "    clf = GridSearchCV(model, param, scoring = ['accuracy','roc_auc_ovr','f1_micro'], refit = False, n_jobs = -1)\n",
    "    best_model = clf.fit(X,y)   \n",
    "    \n",
    "    best_accu = best_model.cv_results_['params'][np.argmin(best_model.cv_results_['rank_test_accuracy'])]\n",
    "    best_roc = best_model.cv_results_['params'][np.argmin(best_model.cv_results_['rank_test_roc_auc_ovr'])]\n",
    "    best_f1 = best_model.cv_results_['params'][np.argmin(best_model.cv_results_['rank_test_f1_micro'])]\n",
    "    \n",
    "    X = df_HTRU_2.iloc[:,:len(df_HTRU_2.columns)-1]\n",
    "    y = df_HTRU_2.iloc[:,len(df_HTRU_2.columns)-1:len(df_HTRU_2.columns)]\n",
    "    y = np.ravel(y)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,train_size = 5000)\n",
    "    \n",
    "    predicteds_accu = [] \n",
    "    predicteds_roc = [] \n",
    "    predicteds_f1 = [] \n",
    "    trueys =[]\n",
    "    \n",
    "    model_accu = RandomForestClassifier(n_estimators = best_accu['n_estimators'], max_features = best_accu['max_features'], min_samples_split = best_accu['min_samples_split'], min_samples_leaf = best_accu['min_samples_leaf'])\n",
    "    model_roc = RandomForestClassifier(n_estimators = best_roc['n_estimators'], max_features = best_roc['max_features'], min_samples_split = best_roc['min_samples_split'], min_samples_leaf = best_roc['min_samples_leaf'])\n",
    "    model_f1 = RandomForestClassifier(n_estimators = best_f1['n_estimators'], max_features = best_f1['max_features'], min_samples_split = best_f1['min_samples_split'], min_samples_leaf = best_f1['min_samples_leaf'])\n",
    "    model_accu.fit(X_train,y_train)\n",
    "    model_roc.fit(X_train,y_train)\n",
    "    model_f1.fit(X_train,y_train)\n",
    "    \n",
    "    print(model_accu.score(X_train,y_train))\n",
    "    \n",
    "    predicteds_accu.append(model_accu.predict(X_test))\n",
    "    predicteds_roc.append(model_roc.predict(X_test))\n",
    "    predicteds_f1.append(model_f1.predict(X_test))\n",
    "    trueys.append(y_test)\n",
    "\n",
    "    predicteds_accu = np.concatenate(predicteds_accu)\n",
    "    predicteds_roc = np.concatenate(predicteds_roc)\n",
    "    predicteds_f1 = np.concatenate(predicteds_f1)\n",
    "    trueys = np.concatenate(trueys)\n",
    "    \n",
    "    accu_score.append(accuracy_score(trueys,predicteds_accu))\n",
    "    roc_score.append(accuracy_score(trueys,predicteds_roc))\n",
    "    f1_score.append(accuracy_score(trueys,predicteds_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9786772117546716, 0.97921997363728, 0.9792975110490811, 0.9804605722260991, 0.9800728851670931]\n",
      "[0.9791424362254788, 0.9804605722260991, 0.978056912460262, 0.9802279599906956, 0.9803054974024967]\n",
      "[0.9787547491664729, 0.97921997363728, 0.9789873614018764, 0.9806156470497015, 0.9800728851670931]\n"
     ]
    }
   ],
   "source": [
    "print(accu_score)\n",
    "print(roc_score)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we proceed to find the best parameters for the connect-4 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.964\n",
      "0.9556\n",
      "0.9692\n",
      "0.964\n",
      "0.9586\n"
     ]
    }
   ],
   "source": [
    "# parameter search for Neural Network for Connect 4\n",
    "param = {'hidden_layer_sizes':[1,2,4,8,32,128],'momentum':[0,0.2,0.5,0.9]}\n",
    "model = MLPClassifier()\n",
    "\n",
    "accu_score = [] \n",
    "roc_score = [] \n",
    "f1_score = [] \n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    data_sample = df_connect_4.sample(5000)\n",
    "    \n",
    "    X = data_sample.iloc[:,:len(data_sample.columns)-1]\n",
    "    y = data_sample.iloc[:,len(data_sample.columns)-1:len(data_sample.columns)]\n",
    "    y = np.ravel(y)\n",
    "\n",
    "    clf = GridSearchCV(model, param, scoring = ['accuracy','roc_auc_ovr','f1_micro'], refit = False, n_jobs = -1)\n",
    "    best_model = clf.fit(X,y)   \n",
    "    \n",
    "    best_accu = best_model.cv_results_['params'][np.argmin(best_model.cv_results_['rank_test_accuracy'])]\n",
    "    best_roc = best_model.cv_results_['params'][np.argmin(best_model.cv_results_['rank_test_roc_auc_ovr'])]\n",
    "    best_f1 = best_model.cv_results_['params'][np.argmin(best_model.cv_results_['rank_test_f1_micro'])]\n",
    "    \n",
    "    X = df_connect_4.iloc[:,:len(df_connect_4.columns)-1]\n",
    "    y = df_connect_4.iloc[:,len(df_connect_4.columns)-1:len(df_connect_4.columns)]\n",
    "    y = np.ravel(y)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,train_size = 5000)\n",
    "    \n",
    "    predicteds_accu = [] \n",
    "    predicteds_roc = [] \n",
    "    predicteds_f1 = [] \n",
    "    trueys =[]\n",
    "    \n",
    "    model_accu = MLPClassifier(hidden_layer_sizes = best_accu['hidden_layer_sizes'], momentum = best_accu['momentum'])\n",
    "    model_roc = MLPClassifier(hidden_layer_sizes = best_roc['hidden_layer_sizes'], momentum = best_roc['momentum'])\n",
    "    model_f1 = MLPClassifier(hidden_layer_sizes = best_f1['hidden_layer_sizes'], momentum = best_f1['momentum'])\n",
    "    model_accu.fit(X_train,y_train)\n",
    "    model_roc.fit(X_train,y_train)\n",
    "    model_f1.fit(X_train,y_train)\n",
    "    \n",
    "    print(model_accu.score(X_train,y_train))\n",
    "    \n",
    "    predicteds_accu.append(model_accu.predict(X_test))\n",
    "    predicteds_roc.append(model_roc.predict(X_test))\n",
    "    predicteds_f1.append(model_f1.predict(X_test))\n",
    "    trueys.append(y_test)\n",
    "\n",
    "    predicteds_accu = np.concatenate(predicteds_accu)\n",
    "    predicteds_roc = np.concatenate(predicteds_roc)\n",
    "    predicteds_f1 = np.concatenate(predicteds_f1)\n",
    "    trueys = np.concatenate(trueys)\n",
    "    \n",
    "    accu_score.append(accuracy_score(trueys,predicteds_accu))\n",
    "    roc_score.append(accuracy_score(trueys,predicteds_roc))\n",
    "    f1_score.append(accuracy_score(trueys,predicteds_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8218716030436729, 0.8153014898650809, 0.8266033633864058, 0.8239337553552017, 0.8180829976341198]\n",
      "[0.8212481616471642, 0.8185945392927937, 0.8248928959652152, 0.8243973399833749, 0.8202730353603171]\n",
      "[0.8227827866231856, 0.8180510262804527, 0.8235500991111964, 0.8244612826907092, 0.822335187671846]\n"
     ]
    }
   ],
   "source": [
    "print(accu_score)\n",
    "print(roc_score)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7876\n",
      "0.7904\n",
      "0.791\n",
      "0.7988\n",
      "0.7934\n"
     ]
    }
   ],
   "source": [
    "# parameter search for Logistic Regression for Connect 4\n",
    "param = {'penalty':('l2','none'), 'C':[0.000001,0.000001,0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000],'solver': ['newton-cg']}\n",
    "model = model = LogisticRegression()\n",
    "\n",
    "accu_score = [] \n",
    "roc_score = [] \n",
    "f1_score = [] \n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    data_sample = df_connect_4.sample(5000)\n",
    "    simplefilter(action='ignore', category=UserWarning)\n",
    "    \n",
    "    X = data_sample.iloc[:,:len(data_sample.columns)-1]\n",
    "    y = data_sample.iloc[:,len(data_sample.columns)-1:len(data_sample.columns)]\n",
    "    y = np.ravel(y)\n",
    "\n",
    "    clf = GridSearchCV(model, param, scoring = ['accuracy','roc_auc_ovr','f1_micro'], refit = False, n_jobs = -1)\n",
    "    best_model = clf.fit(X,y)   \n",
    "    \n",
    "    best_accu = best_model.cv_results_['params'][np.argmin(best_model.cv_results_['rank_test_accuracy'])]\n",
    "    best_roc = best_model.cv_results_['params'][np.argmin(best_model.cv_results_['rank_test_roc_auc_ovr'])]\n",
    "    best_f1 = best_model.cv_results_['params'][np.argmin(best_model.cv_results_['rank_test_f1_micro'])]\n",
    "    \n",
    "    X = df_connect_4.iloc[:,:len(df_connect_4.columns)-1]\n",
    "    y = df_connect_4.iloc[:,len(df_connect_4.columns)-1:len(df_connect_4.columns)]\n",
    "    y = np.ravel(y)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,train_size = 5000)\n",
    "    \n",
    "    predicteds_accu = [] \n",
    "    predicteds_roc = [] \n",
    "    predicteds_f1 = [] \n",
    "    trueys =[]\n",
    "    \n",
    "    model_accu = LogisticRegression(penalty = best_accu['penalty'], C = best_accu['C'], solver = 'newton-cg')\n",
    "    model_roc = LogisticRegression(penalty = best_roc['penalty'], C = best_roc['C'], solver = 'newton-cg')\n",
    "    model_f1 = LogisticRegression(penalty = best_f1['penalty'], C = best_f1['C'], solver = 'newton-cg')\n",
    "    model_accu.fit(X_train,y_train)\n",
    "    model_roc.fit(X_train,y_train)\n",
    "    model_f1.fit(X_train,y_train)\n",
    "    \n",
    "    print(model_accu.score(X_train,y_train))\n",
    "    \n",
    "    predicteds_accu.append(model_accu.predict(X_test))\n",
    "    predicteds_roc.append(model_roc.predict(X_test))\n",
    "    predicteds_f1.append(model_f1.predict(X_test))\n",
    "    trueys.append(y_test)\n",
    "\n",
    "    predicteds_accu = np.concatenate(predicteds_accu)\n",
    "    predicteds_roc = np.concatenate(predicteds_roc)\n",
    "    predicteds_f1 = np.concatenate(predicteds_f1)\n",
    "    trueys = np.concatenate(trueys)\n",
    "    \n",
    "    accu_score.append(accuracy_score(trueys,predicteds_accu))\n",
    "    roc_score.append(accuracy_score(trueys,predicteds_roc))\n",
    "    f1_score.append(accuracy_score(trueys,predicteds_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7843052624848136, 0.785264403094827, 0.7840494916554767, 0.7836338640578042, 0.7865752285951787]\n",
      "[0.7835379499968028, 0.785120532003325, 0.7840494916554767, 0.7833461218748001, 0.7865752285951787]\n",
      "[0.7843052624848136, 0.785264403094827, 0.7840494916554767, 0.7836338640578042, 0.7865752285951787]\n"
     ]
    }
   ],
   "source": [
    "print(accu_score)\n",
    "print(roc_score)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "0.9864\n",
      "0.9884\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# # parameter search for Random Forest for Connect 4\n",
    "param = {'n_estimators':[1024], 'max_features':[1,2,4,6,8], 'min_samples_split':[2,5,10], 'min_samples_leaf':[1,2,4]}\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "accu_score = [] \n",
    "roc_score = [] \n",
    "f1_score = [] \n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    data_sample = df_connect_4.sample(5000)\n",
    "    simplefilter(action='ignore', category=UserWarning)\n",
    "    \n",
    "    X = data_sample.iloc[:,:len(data_sample.columns)-1]\n",
    "    y = data_sample.iloc[:,len(data_sample.columns)-1:len(data_sample.columns)]\n",
    "    y = np.ravel(y)\n",
    "\n",
    "    clf = GridSearchCV(model, param, scoring = ['accuracy','roc_auc_ovr','f1_micro'], refit = False, n_jobs = -1)\n",
    "    best_model = clf.fit(X,y)   \n",
    "    \n",
    "    best_accu = best_model.cv_results_['params'][np.argmin(best_model.cv_results_['rank_test_accuracy'])]\n",
    "    best_roc = best_model.cv_results_['params'][np.argmin(best_model.cv_results_['rank_test_roc_auc_ovr'])]\n",
    "    best_f1 = best_model.cv_results_['params'][np.argmin(best_model.cv_results_['rank_test_f1_micro'])]\n",
    "    \n",
    "    X = df_connect_4.iloc[:,:len(df_connect_4.columns)-1]\n",
    "    y = df_connect_4.iloc[:,len(df_connect_4.columns)-1:len(df_connect_4.columns)]\n",
    "    y = np.ravel(y)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,train_size = 5000)\n",
    "    \n",
    "    predicteds_accu = [] \n",
    "    predicteds_roc = [] \n",
    "    predicteds_f1 = [] \n",
    "    trueys =[]\n",
    "    \n",
    "    model_accu = RandomForestClassifier(n_estimators = best_accu['n_estimators'], max_features = best_accu['max_features'], min_samples_split = best_accu['min_samples_split'], min_samples_leaf = best_accu['min_samples_leaf'])\n",
    "    model_roc = RandomForestClassifier(n_estimators = best_roc['n_estimators'], max_features = best_roc['max_features'], min_samples_split = best_roc['min_samples_split'], min_samples_leaf = best_roc['min_samples_leaf'])\n",
    "    model_f1 = RandomForestClassifier(n_estimators = best_f1['n_estimators'], max_features = best_f1['max_features'], min_samples_split = best_f1['min_samples_split'], min_samples_leaf = best_f1['min_samples_leaf'])\n",
    "    model_accu.fit(X_train,y_train)\n",
    "    model_roc.fit(X_train,y_train)\n",
    "    model_f1.fit(X_train,y_train)\n",
    "    \n",
    "    print(model_accu.score(X_train,y_train))\n",
    "    \n",
    "    predicteds_accu.append(model_accu.predict(X_test))\n",
    "    predicteds_roc.append(model_roc.predict(X_test))\n",
    "    predicteds_f1.append(model_f1.predict(X_test))\n",
    "    trueys.append(y_test)\n",
    "\n",
    "    predicteds_accu = np.concatenate(predicteds_accu)\n",
    "    predicteds_roc = np.concatenate(predicteds_roc)\n",
    "    predicteds_f1 = np.concatenate(predicteds_f1)\n",
    "    trueys = np.concatenate(trueys)\n",
    "    \n",
    "    accu_score.append(accuracy_score(trueys,predicteds_accu))\n",
    "    roc_score.append(accuracy_score(trueys,predicteds_roc))\n",
    "    f1_score.append(accuracy_score(trueys,predicteds_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8231504571903574, 0.8233582709891937, 0.8207366199884903, 0.8249888100262165, 0.820384935098152]\n",
      "[0.8216318178911695, 0.8186105249696272, 0.8209444337873265, 0.8139746786878956, 0.8176034273291131]\n",
      "[0.8231184858366903, 0.8222392736108447, 0.8201291642688151, 0.8246211394590447, 0.8207206343116568]\n"
     ]
    }
   ],
   "source": [
    "print(accu_score)\n",
    "print(roc_score)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we move on to the kr-k dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9964\n",
      "0.9964\n",
      "0.996\n",
      "0.9968\n",
      "0.9964\n"
     ]
    }
   ],
   "source": [
    "# parameter search for Neural Network for King-Rook versus King\n",
    "\n",
    "param = {'hidden_layer_sizes':[1,2,4,8,32,128],'momentum':[0,0.2,0.5,0.9]}\n",
    "model = MLPClassifier()\n",
    "\n",
    "accu_score = [] \n",
    "roc_score = [] \n",
    "f1_score = [] \n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    data_sample = df_krk.sample(5000)\n",
    "    \n",
    "    X = data_sample.iloc[:,:len(data_sample.columns)-1]\n",
    "    y = data_sample.iloc[:,len(data_sample.columns)-1:len(data_sample.columns)]\n",
    "    y = np.ravel(y)\n",
    "\n",
    "    clf = GridSearchCV(model, param, scoring = ['accuracy','roc_auc_ovr','f1_micro'], refit = False, n_jobs = -1)\n",
    "    best_model = clf.fit(X,y)   \n",
    "    \n",
    "    best_accu = best_model.cv_results_['params'][np.argmin(best_model.cv_results_['rank_test_accuracy'])]\n",
    "    best_roc = best_model.cv_results_['params'][np.argmin(best_model.cv_results_['rank_test_roc_auc_ovr'])]\n",
    "    best_f1 = best_model.cv_results_['params'][np.argmin(best_model.cv_results_['rank_test_f1_micro'])]\n",
    "    \n",
    "    X = df_krk.iloc[:,:len(df_krk.columns)-1]\n",
    "    y = df_krk.iloc[:,len(df_krk.columns)-1:len(df_krk.columns)]\n",
    "    y = np.ravel(y)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,train_size = 5000)\n",
    "    \n",
    "    predicteds_accu = [] \n",
    "    predicteds_roc = [] \n",
    "    predicteds_f1 = [] \n",
    "    trueys =[]\n",
    "    \n",
    "    model_accu = MLPClassifier(hidden_layer_sizes = best_accu['hidden_layer_sizes'], momentum = best_accu['momentum'])\n",
    "    model_roc = MLPClassifier(hidden_layer_sizes = best_roc['hidden_layer_sizes'], momentum = best_roc['momentum'])\n",
    "    model_f1 = MLPClassifier(hidden_layer_sizes = best_f1['hidden_layer_sizes'], momentum = best_f1['momentum'])\n",
    "    model_accu.fit(X_train,y_train)\n",
    "    model_roc.fit(X_train,y_train)\n",
    "    model_f1.fit(X_train,y_train)\n",
    "    \n",
    "    print(model_accu.score(X_train,y_train))\n",
    "    \n",
    "    predicteds_accu.append(model_accu.predict(X_test))\n",
    "    predicteds_roc.append(model_roc.predict(X_test))\n",
    "    predicteds_f1.append(model_f1.predict(X_test))\n",
    "    trueys.append(y_test)\n",
    "\n",
    "    predicteds_accu = np.concatenate(predicteds_accu)\n",
    "    predicteds_roc = np.concatenate(predicteds_roc)\n",
    "    predicteds_f1 = np.concatenate(predicteds_f1)\n",
    "    trueys = np.concatenate(trueys)\n",
    "    \n",
    "    accu_score.append(accuracy_score(trueys,predicteds_accu))\n",
    "    roc_score.append(accuracy_score(trueys,predicteds_roc))\n",
    "    f1_score.append(accuracy_score(trueys,predicteds_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9937977099236641, 0.9950988896599584, 0.9949253990284525, 0.9953591256072172, 0.9949687716863289]\n",
      "[0.9936242192921583, 0.9947085357390701, 0.9947085357390701, 0.9961398334489937, 0.9941880638445524]\n",
      "[0.9942748091603053, 0.9953591256072172, 0.9954024982650936, 0.9946651630811936, 0.994882026370576]\n"
     ]
    }
   ],
   "source": [
    "print(accu_score)\n",
    "print(roc_score)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8978\n",
      "0.8996\n",
      "0.9072\n",
      "0.902\n",
      "0.898\n"
     ]
    }
   ],
   "source": [
    "# # parameter search for Logistic Regression for King-Rook versus King\n",
    "param = {'penalty':('l2','none'), 'C':[0.000001,0.000001,0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000],'solver': ['newton-cg']}\n",
    "model = model = LogisticRegression()\n",
    "\n",
    "accu_score = [] \n",
    "roc_score = [] \n",
    "f1_score = [] \n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    data_sample = df_krk.sample(5000)\n",
    "    simplefilter(action='ignore', category=UserWarning)\n",
    "    \n",
    "    X = data_sample.iloc[:,:len(data_sample.columns)-1]\n",
    "    y = data_sample.iloc[:,len(data_sample.columns)-1:len(data_sample.columns)]\n",
    "    y = np.ravel(y)\n",
    "\n",
    "    clf = GridSearchCV(model, param, scoring = ['accuracy','roc_auc_ovr','f1_micro'], refit = False, n_jobs = -1)\n",
    "    best_model = clf.fit(X,y)   \n",
    "    \n",
    "    best_accu = best_model.cv_results_['params'][np.argmin(best_model.cv_results_['rank_test_accuracy'])]\n",
    "    best_roc = best_model.cv_results_['params'][np.argmin(best_model.cv_results_['rank_test_roc_auc_ovr'])]\n",
    "    best_f1 = best_model.cv_results_['params'][np.argmin(best_model.cv_results_['rank_test_f1_micro'])]\n",
    "    \n",
    "    X = df_krk.iloc[:,:len(df_krk.columns)-1]\n",
    "    y = df_krk.iloc[:,len(df_krk.columns)-1:len(df_krk.columns)]\n",
    "    y = np.ravel(y)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,train_size = 5000)\n",
    "    \n",
    "    predicteds_accu = [] \n",
    "    predicteds_roc = [] \n",
    "    predicteds_f1 = [] \n",
    "    trueys =[]\n",
    "    \n",
    "    model_accu = LogisticRegression(penalty = best_accu['penalty'], C = best_accu['C'], solver = 'newton-cg')\n",
    "    model_roc = LogisticRegression(penalty = best_roc['penalty'], C = best_roc['C'], solver = 'newton-cg')\n",
    "    model_f1 = LogisticRegression(penalty = best_f1['penalty'], C = best_f1['C'], solver = 'newton-cg')\n",
    "    model_accu.fit(X_train,y_train)\n",
    "    model_roc.fit(X_train,y_train)\n",
    "    model_f1.fit(X_train,y_train)\n",
    "    \n",
    "    print(model_accu.score(X_train,y_train))\n",
    "    \n",
    "    predicteds_accu.append(model_accu.predict(X_test))\n",
    "    predicteds_roc.append(model_roc.predict(X_test))\n",
    "    predicteds_f1.append(model_f1.predict(X_test))\n",
    "    trueys.append(y_test)\n",
    "\n",
    "    predicteds_accu = np.concatenate(predicteds_accu)\n",
    "    predicteds_roc = np.concatenate(predicteds_roc)\n",
    "    predicteds_f1 = np.concatenate(predicteds_f1)\n",
    "    trueys = np.concatenate(trueys)\n",
    "    \n",
    "    accu_score.append(accuracy_score(trueys,predicteds_accu))\n",
    "    roc_score.append(accuracy_score(trueys,predicteds_roc))\n",
    "    f1_score.append(accuracy_score(trueys,predicteds_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9008934767522554, 0.9005031228313671, 0.898854961832061, 0.8999826509368494, 0.9008501040943789]\n",
      "[0.9008934767522554, 0.9005031228313671, 0.898854961832061, 0.8999826509368494, 0.9008501040943789]\n",
      "[0.9008934767522554, 0.9005031228313671, 0.898854961832061, 0.8999826509368494, 0.9008501040943789]\n"
     ]
    }
   ],
   "source": [
    "print(accu_score)\n",
    "print(roc_score)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# parameter search for Randon Forest for King-Rook versus King\n",
    "param = {'n_estimators':[1024], 'max_features':[1,2,4,6,8], 'min_samples_split':[2,5,10], 'min_samples_leaf':[1,2,4]}\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "accu_score = [] \n",
    "roc_score = [] \n",
    "f1_score = [] \n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    data_sample = df_krk.sample(5000)\n",
    "    simplefilter(action='ignore', category=UserWarning)\n",
    "    \n",
    "    X = data_sample.iloc[:,:len(data_sample.columns)-1]\n",
    "    y = data_sample.iloc[:,len(data_sample.columns)-1:len(data_sample.columns)]\n",
    "    y = np.ravel(y)\n",
    "\n",
    "    clf = GridSearchCV(model, param, scoring = ['accuracy','roc_auc_ovr','f1_micro'], refit = False, n_jobs = -1)\n",
    "    best_model = clf.fit(X,y)   \n",
    "    \n",
    "    best_accu = best_model.cv_results_['params'][np.argmin(best_model.cv_results_['rank_test_accuracy'])]\n",
    "    best_roc = best_model.cv_results_['params'][np.argmin(best_model.cv_results_['rank_test_roc_auc_ovr'])]\n",
    "    best_f1 = best_model.cv_results_['params'][np.argmin(best_model.cv_results_['rank_test_f1_micro'])]\n",
    "    \n",
    "    X = df_krk.iloc[:,:len(df_krk.columns)-1]\n",
    "    y = df_krk.iloc[:,len(df_krk.columns)-1:len(df_krk.columns)]\n",
    "    y = np.ravel(y)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,train_size = 5000)\n",
    "    \n",
    "    predicteds_accu = [] \n",
    "    predicteds_roc = [] \n",
    "    predicteds_f1 = [] \n",
    "    trueys =[]\n",
    "    \n",
    "    model_accu = RandomForestClassifier(n_estimators = best_accu['n_estimators'], max_features = best_accu['max_features'], min_samples_split = best_accu['min_samples_split'], min_samples_leaf = best_accu['min_samples_leaf'])\n",
    "    model_roc = RandomForestClassifier(n_estimators = best_roc['n_estimators'], max_features = best_roc['max_features'], min_samples_split = best_roc['min_samples_split'], min_samples_leaf = best_roc['min_samples_leaf'])\n",
    "    model_f1 = RandomForestClassifier(n_estimators = best_f1['n_estimators'], max_features = best_f1['max_features'], min_samples_split = best_f1['min_samples_split'], min_samples_leaf = best_f1['min_samples_leaf'])\n",
    "    model_accu.fit(X_train,y_train)\n",
    "    model_roc.fit(X_train,y_train)\n",
    "    model_f1.fit(X_train,y_train)\n",
    "    \n",
    "    print(model_accu.score(X_train,y_train))\n",
    "    \n",
    "    predicteds_accu.append(model_accu.predict(X_test))\n",
    "    predicteds_roc.append(model_roc.predict(X_test))\n",
    "    predicteds_f1.append(model_f1.predict(X_test))\n",
    "    trueys.append(y_test)\n",
    "\n",
    "    predicteds_accu = np.concatenate(predicteds_accu)\n",
    "    predicteds_roc = np.concatenate(predicteds_roc)\n",
    "    predicteds_f1 = np.concatenate(predicteds_f1)\n",
    "    trueys = np.concatenate(trueys)\n",
    "    \n",
    "    accu_score.append(accuracy_score(trueys,predicteds_accu))\n",
    "    roc_score.append(accuracy_score(trueys,predicteds_roc))\n",
    "    f1_score.append(accuracy_score(trueys,predicteds_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9882893823733518, 0.9901977793199167, 0.9858605135322692, 0.9876387925052047, 0.9773160999306038]\n",
      "[0.9874219292158224, 0.9899375433726578, 0.9857737682165163, 0.9875954198473282, 0.976969118667592]\n",
      "[0.9884628730048577, 0.9901977793199167, 0.9857303955586398, 0.9874219292158224, 0.9767956280360861]\n"
     ]
    }
   ],
   "source": [
    "print(accu_score)\n",
    "print(roc_score)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then focus on the adult census income dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7898\n",
      "0.7834\n",
      "0.7474\n",
      "0.7918\n",
      "0.7896\n"
     ]
    }
   ],
   "source": [
    "# parameter search for Neural Network for Adult Income\n",
    "param = {'hidden_layer_sizes':[1,2,4,8,32,128],'momentum':[0,0.2,0.5,0.9]}\n",
    "model = MLPClassifier()\n",
    "\n",
    "accu_score = [] \n",
    "roc_score = [] \n",
    "f1_score = [] \n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    data_sample = df_adult_income.sample(5000)\n",
    "    \n",
    "    X = data_sample.iloc[:,:len(data_sample.columns)-1]\n",
    "    y = data_sample.iloc[:,len(data_sample.columns)-1:len(data_sample.columns)]\n",
    "    y = np.ravel(y)\n",
    "\n",
    "    clf = GridSearchCV(model, param, scoring = ['accuracy','roc_auc_ovr','f1_micro'], refit = False, n_jobs = -1)\n",
    "    best_model = clf.fit(X,y)   \n",
    "    \n",
    "    best_accu = best_model.cv_results_['params'][np.argmin(best_model.cv_results_['rank_test_accuracy'])]\n",
    "    best_roc = best_model.cv_results_['params'][np.argmin(best_model.cv_results_['rank_test_roc_auc_ovr'])]\n",
    "    best_f1 = best_model.cv_results_['params'][np.argmin(best_model.cv_results_['rank_test_f1_micro'])]\n",
    "    \n",
    "    X = df_adult_income.iloc[:,:len(df_adult_income.columns)-1]\n",
    "    y = df_adult_income.iloc[:,len(df_adult_income.columns)-1:len(df_adult_income.columns)]\n",
    "    y = np.ravel(y)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,train_size = 5000)\n",
    "    \n",
    "    predicteds_accu = [] \n",
    "    predicteds_roc = [] \n",
    "    predicteds_f1 = [] \n",
    "    trueys =[]\n",
    "    \n",
    "    model_accu = MLPClassifier(hidden_layer_sizes = best_accu['hidden_layer_sizes'], momentum = best_accu['momentum'])\n",
    "    model_roc = MLPClassifier(hidden_layer_sizes = best_roc['hidden_layer_sizes'], momentum = best_roc['momentum'])\n",
    "    model_f1 = MLPClassifier(hidden_layer_sizes = best_f1['hidden_layer_sizes'], momentum = best_f1['momentum'])\n",
    "    model_accu.fit(X_train,y_train)\n",
    "    model_roc.fit(X_train,y_train)\n",
    "    model_f1.fit(X_train,y_train)\n",
    "    \n",
    "    print(model_accu.score(X_train,y_train))\n",
    "    \n",
    "    predicteds_accu.append(model_accu.predict(X_test))\n",
    "    predicteds_roc.append(model_roc.predict(X_test))\n",
    "    predicteds_f1.append(model_f1.predict(X_test))\n",
    "    trueys.append(y_test)\n",
    "\n",
    "    predicteds_accu = np.concatenate(predicteds_accu)\n",
    "    predicteds_roc = np.concatenate(predicteds_roc)\n",
    "    predicteds_f1 = np.concatenate(predicteds_f1)\n",
    "    trueys = np.concatenate(trueys)\n",
    "    \n",
    "    accu_score.append(accuracy_score(trueys,predicteds_accu))\n",
    "    roc_score.append(accuracy_score(trueys,predicteds_roc))\n",
    "    f1_score.append(accuracy_score(trueys,predicteds_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7879249664380829, 0.7882515148216683, 0.7389789920539893, 0.7875984180544973, 0.7859656761365698]\n",
      "[0.23975907985922137, 0.7942382351874024, 0.7807409019992018, 0.7829904575305685, 0.7897028409709372]\n",
      "[0.7903559377381082, 0.792206378578426, 0.7933311563441094, 0.7895939915097421, 0.7934037226515729]\n"
     ]
    }
   ],
   "source": [
    "print(accu_score)\n",
    "print(roc_score)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\10841\\anaconda3\\lib\\site-packages\\scipy\\optimize\\linesearch.py:477: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "C:\\Users\\10841\\anaconda3\\lib\\site-packages\\scipy\\optimize\\linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8518\n",
      "0.8496\n",
      "0.861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\10841\\anaconda3\\lib\\site-packages\\scipy\\optimize\\linesearch.py:477: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "C:\\Users\\10841\\anaconda3\\lib\\site-packages\\scipy\\optimize\\linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.854\n",
      "0.861\n"
     ]
    }
   ],
   "source": [
    "# parameter search for Logistic Regression for Adult Income\n",
    "param = {'penalty':('l2','none'), 'C':[0.000001,0.000001,0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000],'solver': ['newton-cg']}\n",
    "model = model = LogisticRegression()\n",
    "\n",
    "accu_score = [] \n",
    "roc_score = [] \n",
    "f1_score = [] \n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    data_sample = df_adult_income.sample(5000)\n",
    "    simplefilter(action='ignore', category=UserWarning)\n",
    "    \n",
    "    X = data_sample.iloc[:,:len(data_sample.columns)-1]\n",
    "    y = data_sample.iloc[:,len(data_sample.columns)-1:len(data_sample.columns)]\n",
    "    y = np.ravel(y)\n",
    "\n",
    "    clf = GridSearchCV(model, param, scoring = ['accuracy','roc_auc_ovr','f1_micro'], refit = False, n_jobs = -1)\n",
    "    best_model = clf.fit(X,y)   \n",
    "    \n",
    "    best_accu = best_model.cv_results_['params'][np.argmin(best_model.cv_results_['rank_test_accuracy'])]\n",
    "    best_roc = best_model.cv_results_['params'][np.argmin(best_model.cv_results_['rank_test_roc_auc_ovr'])]\n",
    "    best_f1 = best_model.cv_results_['params'][np.argmin(best_model.cv_results_['rank_test_f1_micro'])]\n",
    "    \n",
    "    X = df_adult_income.iloc[:,:len(df_adult_income.columns)-1]\n",
    "    y = df_adult_income.iloc[:,len(df_adult_income.columns)-1:len(df_adult_income.columns)]\n",
    "    y = np.ravel(y)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,train_size = 5000)\n",
    "    \n",
    "    predicteds_accu = [] \n",
    "    predicteds_roc = [] \n",
    "    predicteds_f1 = [] \n",
    "    trueys =[]\n",
    "    \n",
    "    model_accu = LogisticRegression(penalty = best_accu['penalty'], C = best_accu['C'], solver = 'newton-cg')\n",
    "    model_roc = LogisticRegression(penalty = best_roc['penalty'], C = best_roc['C'], solver = 'newton-cg')\n",
    "    model_f1 = LogisticRegression(penalty = best_f1['penalty'], C = best_f1['C'], solver = 'newton-cg')\n",
    "    model_accu.fit(X_train,y_train)\n",
    "    model_roc.fit(X_train,y_train)\n",
    "    model_f1.fit(X_train,y_train)\n",
    "    \n",
    "    print(model_accu.score(X_train,y_train))\n",
    "    \n",
    "    predicteds_accu.append(model_accu.predict(X_test))\n",
    "    predicteds_roc.append(model_roc.predict(X_test))\n",
    "    predicteds_f1.append(model_f1.predict(X_test))\n",
    "    trueys.append(y_test)\n",
    "\n",
    "    predicteds_accu = np.concatenate(predicteds_accu)\n",
    "    predicteds_roc = np.concatenate(predicteds_roc)\n",
    "    predicteds_f1 = np.concatenate(predicteds_f1)\n",
    "    trueys = np.concatenate(trueys)\n",
    "    \n",
    "    accu_score.append(accuracy_score(trueys,predicteds_accu))\n",
    "    roc_score.append(accuracy_score(trueys,predicteds_roc))\n",
    "    f1_score.append(accuracy_score(trueys,predicteds_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8500780087805232, 0.849969159319328, 0.8506948223939624, 0.8494974783208157, 0.8500054424730598]\n",
      "[0.8498240267044012, 0.848916947861108, 0.8506948223939624, 0.8499328761655963, 0.8494249120133522]\n",
      "[0.8500780087805232, 0.849969159319328, 0.8506948223939624, 0.8494974783208157, 0.8500054424730598]\n"
     ]
    }
   ],
   "source": [
    "print(accu_score)\n",
    "print(roc_score)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9002\n",
      "0.9028\n",
      "0.9032\n",
      "0.8932\n",
      "0.8966\n"
     ]
    }
   ],
   "source": [
    "# parameter search for Random Forest for Adult Income\n",
    "param = {'n_estimators':[1024], 'max_features':[1,2,4,6,8], 'min_samples_split':[2,5,10], 'min_samples_leaf':[1,2,4]}\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "accu_score = [] \n",
    "roc_score = [] \n",
    "f1_score = [] \n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    data_sample = df_adult_income.sample(5000)\n",
    "    simplefilter(action='ignore', category=UserWarning)\n",
    "    \n",
    "    X = data_sample.iloc[:,:len(data_sample.columns)-1]\n",
    "    y = data_sample.iloc[:,len(data_sample.columns)-1:len(data_sample.columns)]\n",
    "    y = np.ravel(y)\n",
    "\n",
    "    clf = GridSearchCV(model, param, scoring = ['accuracy','roc_auc_ovr','f1_micro'], refit = False, n_jobs = -1)\n",
    "    best_model = clf.fit(X,y)   \n",
    "    \n",
    "    best_accu = best_model.cv_results_['params'][np.argmin(best_model.cv_results_['rank_test_accuracy'])]\n",
    "    best_roc = best_model.cv_results_['params'][np.argmin(best_model.cv_results_['rank_test_roc_auc_ovr'])]\n",
    "    best_f1 = best_model.cv_results_['params'][np.argmin(best_model.cv_results_['rank_test_f1_micro'])]\n",
    "    \n",
    "    X = df_adult_income.iloc[:,:len(df_adult_income.columns)-1]\n",
    "    y = df_adult_income.iloc[:,len(df_adult_income.columns)-1:len(df_adult_income.columns)]\n",
    "    y = np.ravel(y)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,train_size = 5000)\n",
    "    \n",
    "    predicteds_accu = [] \n",
    "    predicteds_roc = [] \n",
    "    predicteds_f1 = [] \n",
    "    trueys =[]\n",
    "    \n",
    "    model_accu = RandomForestClassifier(n_estimators = best_accu['n_estimators'], max_features = best_accu['max_features'], min_samples_split = best_accu['min_samples_split'], min_samples_leaf = best_accu['min_samples_leaf'])\n",
    "    model_roc = RandomForestClassifier(n_estimators = best_roc['n_estimators'], max_features = best_roc['max_features'], min_samples_split = best_roc['min_samples_split'], min_samples_leaf = best_roc['min_samples_leaf'])\n",
    "    model_f1 = RandomForestClassifier(n_estimators = best_f1['n_estimators'], max_features = best_f1['max_features'], min_samples_split = best_f1['min_samples_split'], min_samples_leaf = best_f1['min_samples_leaf'])\n",
    "    model_accu.fit(X_train,y_train)\n",
    "    model_roc.fit(X_train,y_train)\n",
    "    model_f1.fit(X_train,y_train)\n",
    "    \n",
    "    print(model_accu.score(X_train,y_train))\n",
    "    \n",
    "    predicteds_accu.append(model_accu.predict(X_test))\n",
    "    predicteds_roc.append(model_roc.predict(X_test))\n",
    "    predicteds_f1.append(model_f1.predict(X_test))\n",
    "    trueys.append(y_test)\n",
    "\n",
    "    predicteds_accu = np.concatenate(predicteds_accu)\n",
    "    predicteds_roc = np.concatenate(predicteds_roc)\n",
    "    predicteds_f1 = np.concatenate(predicteds_f1)\n",
    "    trueys = np.concatenate(trueys)\n",
    "    \n",
    "    accu_score.append(accuracy_score(trueys,predicteds_accu))\n",
    "    roc_score.append(accuracy_score(trueys,predicteds_roc))\n",
    "    f1_score.append(accuracy_score(trueys,predicteds_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8562824280686477, 0.8594753455970393, 0.8592213635209173, 0.8570080911432821, 0.8533797757701099]\n",
      "[0.8570443742970139, 0.8588948151373318, 0.8598381771343565, 0.8567541090671601, 0.851674467544719]\n",
      "[0.8563549943761112, 0.8589673814447952, 0.8594027792895759, 0.8565364101447698, 0.8534160589238416]\n"
     ]
    }
   ],
   "source": [
    "print(accu_score)\n",
    "print(roc_score)\n",
    "print(f1_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
